{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Intro to SQL (and BigQuery)\n",
    "- https://www.kaggle.com/learn/intro-to-sql"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Select, From & Where\n",
    "- The foundational compontents for all SQL queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Select the `Nmae` column from `pets` table in the `pet_records` database in the `bigquery-public-data` project:\n",
    "- query='''    \n",
    "     SELECT Name    \n",
    "     FROM `bigwuery-public-data.pet_records.pets`\n",
    "     '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "- What are all the U.S. cities in the OpenAQ dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://googleapis.dev/python/google-api-core/latest/auth.html\n",
    "# from google.oauth2 import service_account\n",
    "\n",
    "# json_account_info = #{service_account_info}\n",
    "# credentials = service_account.Credentials.from_service_account_info(\n",
    "#     json_account_info)\n",
    "# ...\n",
    "\n",
    "# from google.cloud import bigquery\n",
    "# client = bigquery.Client(credentials=credentials)\n",
    "\n",
    "# sql=\"select * from austin_311.311_service_requests\"\n",
    "# query_job=client.query(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jm\\anaconda3\\envs\\KglSQL_1\\lib\\site-packages\\google\\auth\\_default.py:70: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "# Create a 'Client' Object.\n",
    "client = bigquery.Client('1010155882053')\n",
    "#gcloud projects list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a reference to the \"openaq\" dataset\n",
    "dataset_ref = client.dataset(\"openaq\", project=\"bigquery-public-data\")\n",
    "# API request - fetch the dataset\n",
    "dataset = client.get_dataset(dataset_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_air_quality\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<google.cloud.bigquery.table.TableListItem at 0x18033805660>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all the tables in the \"hacker_news\" dataset\n",
    "tables = list(client.list_tables(dataset))\n",
    "# Print names of all tables in the dataset (there are four!)\n",
    "for table in tables:  \n",
    "    print(table.table_id)\n",
    "\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SchemaField('location', 'STRING', 'NULLABLE', None, (), None),\n",
       " SchemaField('city', 'STRING', 'NULLABLE', None, (), None),\n",
       " SchemaField('country', 'STRING', 'NULLABLE', None, (), None),\n",
       " SchemaField('pollutant', 'STRING', 'NULLABLE', None, (), None),\n",
       " SchemaField('value', 'FLOAT', 'NULLABLE', None, (), None),\n",
       " SchemaField('timestamp', 'TIMESTAMP', 'NULLABLE', None, (), None),\n",
       " SchemaField('unit', 'STRING', 'NULLABLE', None, (), None),\n",
       " SchemaField('source_name', 'STRING', 'NULLABLE', None, (), None),\n",
       " SchemaField('latitude', 'FLOAT', 'NULLABLE', None, (), None),\n",
       " SchemaField('longitude', 'FLOAT', 'NULLABLE', None, (), None),\n",
       " SchemaField('averaged_over_in_hours', 'FLOAT', 'NULLABLE', None, (), None),\n",
       " SchemaField('location_geom', 'GEOGRAPHY', 'NULLABLE', None, (), None)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct a reference to the \"global_air_quality\" table\n",
    "table_ref = dataset_ref.table(\"global_air_quality\")\n",
    "# API request - fetch the table\n",
    "table = client.get_table(table_ref)\n",
    "\n",
    "table.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jm\\AppData\\Local\\Temp\\ipykernel_304\\2647132754.py:2: UserWarning: Cannot use bqstorage_client if max_results is set, reverting to fetching data with the tabledata.list endpoint.\n",
      "  client.list_rows(table, max_results=5).to_dataframe()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>pollutant</th>\n",
       "      <th>value</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>unit</th>\n",
       "      <th>source_name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>averaged_over_in_hours</th>\n",
       "      <th>location_geom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Borówiec, ul. Drapałka</td>\n",
       "      <td>Borówiec</td>\n",
       "      <td>PL</td>\n",
       "      <td>bc</td>\n",
       "      <td>0.85217</td>\n",
       "      <td>2022-04-28 07:00:00+00:00</td>\n",
       "      <td>µg/m³</td>\n",
       "      <td>GIOS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.276794</td>\n",
       "      <td>17.074114</td>\n",
       "      <td>POINT(52.276794 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kraków, ul. Bulwarowa</td>\n",
       "      <td>Kraków</td>\n",
       "      <td>PL</td>\n",
       "      <td>bc</td>\n",
       "      <td>0.91284</td>\n",
       "      <td>2022-04-27 23:00:00+00:00</td>\n",
       "      <td>µg/m³</td>\n",
       "      <td>GIOS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.069308</td>\n",
       "      <td>20.053492</td>\n",
       "      <td>POINT(50.069308 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Płock, ul. Reja</td>\n",
       "      <td>Płock</td>\n",
       "      <td>PL</td>\n",
       "      <td>bc</td>\n",
       "      <td>1.41000</td>\n",
       "      <td>2022-03-30 04:00:00+00:00</td>\n",
       "      <td>µg/m³</td>\n",
       "      <td>GIOS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.550938</td>\n",
       "      <td>19.709791</td>\n",
       "      <td>POINT(52.550938 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elbląg, ul. Bażyńskiego</td>\n",
       "      <td>Elbląg</td>\n",
       "      <td>PL</td>\n",
       "      <td>bc</td>\n",
       "      <td>0.33607</td>\n",
       "      <td>2022-05-03 13:00:00+00:00</td>\n",
       "      <td>µg/m³</td>\n",
       "      <td>GIOS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54.167847</td>\n",
       "      <td>19.410942</td>\n",
       "      <td>POINT(54.167847 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Piastów, ul. Pułaskiego</td>\n",
       "      <td>Piastów</td>\n",
       "      <td>PL</td>\n",
       "      <td>bc</td>\n",
       "      <td>0.51000</td>\n",
       "      <td>2022-05-11 05:00:00+00:00</td>\n",
       "      <td>µg/m³</td>\n",
       "      <td>GIOS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.191728</td>\n",
       "      <td>20.837489</td>\n",
       "      <td>POINT(52.191728 1)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  location      city country pollutant    value  \\\n",
       "0   Borówiec, ul. Drapałka  Borówiec      PL        bc  0.85217   \n",
       "1    Kraków, ul. Bulwarowa    Kraków      PL        bc  0.91284   \n",
       "2          Płock, ul. Reja     Płock      PL        bc  1.41000   \n",
       "3  Elbląg, ul. Bażyńskiego    Elbląg      PL        bc  0.33607   \n",
       "4  Piastów, ul. Pułaskiego   Piastów      PL        bc  0.51000   \n",
       "\n",
       "                  timestamp   unit source_name  latitude  longitude  \\\n",
       "0 2022-04-28 07:00:00+00:00  µg/m³        GIOS       1.0  52.276794   \n",
       "1 2022-04-27 23:00:00+00:00  µg/m³        GIOS       1.0  50.069308   \n",
       "2 2022-03-30 04:00:00+00:00  µg/m³        GIOS       1.0  52.550938   \n",
       "3 2022-05-03 13:00:00+00:00  µg/m³        GIOS       1.0  54.167847   \n",
       "4 2022-05-11 05:00:00+00:00  µg/m³        GIOS       1.0  52.191728   \n",
       "\n",
       "   averaged_over_in_hours       location_geom  \n",
       "0               17.074114  POINT(52.276794 1)  \n",
       "1               20.053492  POINT(50.069308 1)  \n",
       "2               19.709791  POINT(52.550938 1)  \n",
       "3               19.410942  POINT(54.167847 1)  \n",
       "4               20.837489  POINT(52.191728 1)  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a peek at the first few rows\n",
    "client.list_rows(table, max_results=5).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### JM mk a DF from the table...\n",
    "#df = client.list_rows(table).to_dataframe()  # 403 Permission denied on resource project jmproject.\n",
    "#df = client.list_rows(table, max_results=10000).to_dataframe()   # ok but only 10_000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ok w/Data now mk a Query\n",
    "# Query to select all the items from the \"city\" column where the \"country\" column is 'US'\n",
    "query = \"\"\"\n",
    "        SELECT city\n",
    "        FROM `bigquery-public-data.openaq.global_air_quality`\n",
    "        WHERE country = 'US'\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting the query to the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\r\n",
    "We begin by setting up the query with the query() method. We run the method with the default parameters, but this method also allows us to specify more complicated settings that you can read about in the documentation. We'll revisit this late\n",
    "- https://cloud.google.com/python/docs/reference/bigquery/latest/google.cloud.bigquery.client.Client\n",
    "- https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#jobconfigurationquery\n",
    "- https://cloud.google.com/bigquery/docs/quickstarts/quickstart-client-libraries.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_job = client.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API request - run the query, and return a pandas DataFrame\n",
    "us_cities = query_job.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\r\n",
    "Now we've got a pandas DataFrame called us_cities, which we can use like any other DataFram.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "city\n",
       "Phoenix-Mesa-Scottsdale                     39414\n",
       "Los Angeles-Long Beach-Santa Ana            27479\n",
       "Riverside-San Bernardino-Ontario            26887\n",
       "New York-Northern New Jersey-Long Island    25417\n",
       "San Francisco-Oakland-Fremont               22710\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What five cities have the most measurements?\n",
    "us_cities.city.value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More queries\n",
    "- If you want multiple columns, you can select them with a comma between the names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "        SELECT city, country\n",
    "        FROM `bigquery-public-data.openaq.global_air_quality`\n",
    "        WHERE country = 'US'\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All columns with a *:\n",
    "query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM `bigquery-public-data.openaq.global_air_quality`\n",
    "        WHERE country = 'US'\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with big datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BigQuery datasets can be huge. We allow you to do a lot of computation for free, but everyone has some limit.\r\n",
    "\r\n",
    "Each Kaggle user can scan 5TB every 30 days for free. Once you hit that limit, you'll have to wait for it to reset.\r\n",
    "\r\n",
    "The biggest dataset currently on Kaggle is 3TB, so you can go through your 30-day limit in a couple queries if you aren't careful.\r\n",
    "\r\n",
    "Don't worry though: we'll teach you how to avoid scanning too much data at once, so that you don't run over your limit.\r\n",
    "\r\n",
    "To begin,you can estimate the size of any query before running it. Here is an example using the (very large!) Hacker News dataset. To see how much data a query will scan, we create a QueryJobConfig object and set the dry_run parameter to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This query will process 553320240 bytes.\n"
     ]
    }
   ],
   "source": [
    "# Query to get the score column from every row where the type column has value \"job\"\n",
    "query = \"\"\"\n",
    "        SELECT score, title\n",
    "        FROM `bigquery-public-data.hacker_news.full`\n",
    "        WHERE type = \"job\" \n",
    "        \"\"\"\n",
    "\n",
    "# Create a QueryJobConfig object to estimate size of query without running it\n",
    "dry_run_config = bigquery.QueryJobConfig(dry_run=True)\n",
    "\n",
    "# API request - dry run query to estimate costs\n",
    "dry_run_query_job = client.query(query, job_config=dry_run_config)\n",
    "\n",
    "print(\"This query will process {} bytes.\".format(dry_run_query_job.total_bytes_processed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\r\n",
    "You can also specify a parameter when running the query to limit how much data you are willing to scan. Here's an example with a low limi.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalServerError",
     "evalue": "500 Query exceeded limit for bytes billed: 1000000. 553648128 or higher required.\n\n(job ID: fdbe33ce-654a-4413-b26b-554fab315098)\n\n             -----Query Job SQL Follows-----             \n\n    |    .    |    .    |    .    |    .    |    .    |\n   1:\n   2:        SELECT score, title\n   3:        FROM `bigquery-public-data.hacker_news.full`\n   4:        WHERE type = \"job\" \n   5:        \n    |    .    |    .    |    .    |    .    |    .    |",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m safe_query_job \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mquery(query, job_config\u001b[38;5;241m=\u001b[39msafe_config)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# API request - try to run the query, and return a pandas DataFrame\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43msafe_query_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\KglSQL_1\\lib\\site-packages\\google\\cloud\\bigquery\\job.py:3401\u001b[0m, in \u001b[0;36mQueryJob.to_dataframe\u001b[1;34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, date_as_object)\u001b[0m\n\u001b[0;32m   3340\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dataframe\u001b[39m(\n\u001b[0;32m   3341\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3342\u001b[0m     bqstorage_client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3346\u001b[0m     date_as_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   3347\u001b[0m ):\n\u001b[0;32m   3348\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a pandas DataFrame from a QueryJob\u001b[39;00m\n\u001b[0;32m   3349\u001b[0m \n\u001b[0;32m   3350\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3399\u001b[0m \u001b[38;5;124;03m        ValueError: If the `pandas` library cannot be imported.\u001b[39;00m\n\u001b[0;32m   3400\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_dataframe(\n\u001b[0;32m   3402\u001b[0m         bqstorage_client\u001b[38;5;241m=\u001b[39mbqstorage_client,\n\u001b[0;32m   3403\u001b[0m         dtypes\u001b[38;5;241m=\u001b[39mdtypes,\n\u001b[0;32m   3404\u001b[0m         progress_bar_type\u001b[38;5;241m=\u001b[39mprogress_bar_type,\n\u001b[0;32m   3405\u001b[0m         create_bqstorage_client\u001b[38;5;241m=\u001b[39mcreate_bqstorage_client,\n\u001b[0;32m   3406\u001b[0m         date_as_object\u001b[38;5;241m=\u001b[39mdate_as_object,\n\u001b[0;32m   3407\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\KglSQL_1\\lib\\site-packages\\google\\cloud\\bigquery\\job.py:3230\u001b[0m, in \u001b[0;36mQueryJob.result\u001b[1;34m(self, page_size, max_results, retry, timeout, start_index)\u001b[0m\n\u001b[0;32m   3193\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Start the job and wait for it to complete and get the result.\u001b[39;00m\n\u001b[0;32m   3194\u001b[0m \n\u001b[0;32m   3195\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3227\u001b[0m \u001b[38;5;124;03m        If the job did not complete in the given timeout.\u001b[39;00m\n\u001b[0;32m   3228\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3230\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mQueryJob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3232\u001b[0m     \u001b[38;5;66;03m# Return an iterator instead of returning the job.\u001b[39;00m\n\u001b[0;32m   3233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query_results:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\KglSQL_1\\lib\\site-packages\\google\\cloud\\bigquery\\job.py:835\u001b[0m, in \u001b[0;36m_AsyncJob.result\u001b[1;34m(self, retry, timeout)\u001b[0m\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_begin(retry\u001b[38;5;241m=\u001b[39mretry, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    834\u001b[0m \u001b[38;5;66;03m# TODO: modify PollingFuture so it can pass a retry argument to done().\u001b[39;00m\n\u001b[1;32m--> 835\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_AsyncJob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\KglSQL_1\\lib\\site-packages\\google\\api_core\\future\\polling.py:135\u001b[0m, in \u001b[0;36mPollingFuture.result\u001b[1;34m(self, timeout, retry)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking_poll(timeout\u001b[38;5;241m=\u001b[39mtimeout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# pylint: disable=raising-bad-type\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;66;03m# Pylint doesn't recognize that this is valid in this case.\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "\u001b[1;31mInternalServerError\u001b[0m: 500 Query exceeded limit for bytes billed: 1000000. 553648128 or higher required.\n\n(job ID: fdbe33ce-654a-4413-b26b-554fab315098)\n\n             -----Query Job SQL Follows-----             \n\n    |    .    |    .    |    .    |    .    |    .    |\n   1:\n   2:        SELECT score, title\n   3:        FROM `bigquery-public-data.hacker_news.full`\n   4:        WHERE type = \"job\" \n   5:        \n    |    .    |    .    |    .    |    .    |    .    |"
     ]
    }
   ],
   "source": [
    "# Only run the query if it's less than 1 MB\n",
    "ONE_MB = 1000*1000\n",
    "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=ONE_MB)\n",
    "\n",
    "# Set up the query (will only run if it's less than 1 MB)\n",
    "safe_query_job = client.query(query, job_config=safe_config)\n",
    "\n",
    "# API request - try to run the query, and return a pandas DataFrame\n",
    "safe_query_job.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\r\n",
    "In this case, the query was cancelled, because the limit of 1 MB was exceeded. However, we can increase the limit to run the query successfull!\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7267060367454068"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only run the query if it's less than 1 GB\n",
    "ONE_GB = 1000*1000*1000\n",
    "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=ONE_GB)\n",
    "\n",
    "# Set up the query (will only run if it's less than 1 GB)\n",
    "safe_query_job = client.query(query, job_config=safe_config)\n",
    "\n",
    "# API request - try to run the query, and return a pandas DataFrame\n",
    "job_post_scores = safe_query_job.to_dataframe()\n",
    "\n",
    "# Print average score for job posts\n",
    "job_post_scores.score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
